# lexical_analyzer

Documentation v0.001

1. Problem Statement

The task for this assignment was to program a lexical analyzer (lexer) that reads a user-inputted text file and generates tokens based on the contents of the file.

2. How to use your program

0) (if needed) Put text file(s) you want to lexically analyze in the same folder as lexer.exe
1) Run lexer.exe
2) Type name of the text file, e.g. sample.txt
3) Press enter
4) View results in terminal



3. Design of your program

Our program uses a 2D-array as a state table which maintains rules for valid and invalid values to correctly distingush between different types of tokens. We used a Token struct which holds attributes of the lexeme being processed including its position on the state table. The lexical analyzer first prompts the user to enter the name of a text file, with error checks for if the file is invalid or missing. The analyzer processes an input file one character at a time, one line at a time. For each new line a vector of tokens is generated by calling lexer(), and each corresponding token and lexeme printed out by the console until the end of the file is reached.


Hardcoded sets for the token types SEPERATORS, OPERATORS, SPECIALS, KEYWORDS were used.
No sorting algorithms were used; only file traversal, vector iteration, array indexing and searching.


Lexer() iterates through each character in a line, generating its state using on the helper function getCharState(). This function determines the state of the current token based on the current character and previous state. This state is fed back into lexer() and assigned in a value in accordance to the state table rules. If the value is valid, the current character is appended to the current token's lexeme with helper function getLexemeName(). If the value is rejected by the state table, we know that the previous token ended at the previous character and the current character is the start of a new token. This way we account for parsing boundaries and are able to correctly separate tokens that are not separated by whitespace. Checks are also made for detecting whitespaces and new lines so that ends of 'words' are not cut off from a lexeme.


After lexical analysis concludes the ifstream is flushed from memory and the program successfully terminates.


4. Any Limitation

None

Tested and works with a 1 million line text file (51MB), although it takes several minutes to compute.

yes "Filler text () {12.5}. int num_00 123; STDinput..." | head -n 1000000 > giant.txt




5. Any shortcomings

None


# Source
https://github.com/KirbyNguyen/lexical_analyzer
